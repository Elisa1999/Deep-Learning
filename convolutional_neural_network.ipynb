{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elisa1999/Deep-Learning/blob/main/convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q dataset.zip -d dataset "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld_Aq86p1sjP",
        "outputId": "72d29dac-ff0e-4656-c56c-4d12653a8bb4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace dataset/__MACOSX/._dataset? [y]es, [n]o, [A]ll, [N]one, [r]ename: All\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "3fByOuNJLElR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__ #check the version of tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "ExkF8QwfLT-3",
        "outputId": "31f41514-aea4-4757-c7bc-7e77ec88d3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#apply some transformations on the training set -> avoid overfitting"
      ],
      "metadata": {
        "id": "ANnr3li7LTKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image augmatation -> the model will not overlearn, augment the diversity of the image\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255, #feature rescaling dividing the pixel by 255 (nomarlization)\n",
        "        shear_range=0.2, \n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'dataset/dataset/training_set',  # this is the target directory\n",
        "        target_size=(64, 64),  # all images will be resized to 150x150\n",
        "        batch_size = 32,#default value\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "1o7AgvEXMD06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430b938a-5f0a-4951-a728-468c056c3386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255) #we dont want to touch the training set, only rescale the pixel\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'dataset/dataset/test_set',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "z83M4vfqOf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b563809-0c45-4c1d-a1e8-2a19ddf744e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "VfxgwiBJPycM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64,64,3]))"
      ],
      "metadata": {
        "id": "3axPuXZ1P2rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2,2), strides = 2))"
      ],
      "metadata": {
        "id": "Im0rO0ktRIb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2,2), strides = 2))"
      ],
      "metadata": {
        "id": "rPAZ7FIISB65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "O7HdVAXNSXBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
      ],
      "metadata": {
        "id": "2T1P6q_ASbcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units = 1, activation ='sigmoid')) #binary classification/if multiclass -> softmax"
      ],
      "metadata": {
        "id": "iDgYUseISb2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'BinaryCrossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "DjHgYbwyTZll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25 )"
      ],
      "metadata": {
        "id": "Cs0iy1pxTw47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f44917-6ce2-4e7a-b66e-54919749b6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 68s 269ms/step - loss: 0.6537 - accuracy: 0.6076 - val_loss: 0.6085 - val_accuracy: 0.6730\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.5941 - accuracy: 0.6827 - val_loss: 0.5890 - val_accuracy: 0.6955\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 67s 268ms/step - loss: 0.5589 - accuracy: 0.7145 - val_loss: 0.7192 - val_accuracy: 0.6070\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 67s 268ms/step - loss: 0.5202 - accuracy: 0.7446 - val_loss: 0.5048 - val_accuracy: 0.7545\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.4988 - accuracy: 0.7561 - val_loss: 0.4852 - val_accuracy: 0.7790\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.4903 - accuracy: 0.7558 - val_loss: 0.4787 - val_accuracy: 0.7695\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.4731 - accuracy: 0.7741 - val_loss: 0.4836 - val_accuracy: 0.7660\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.4585 - accuracy: 0.7804 - val_loss: 0.4602 - val_accuracy: 0.7865\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.4484 - accuracy: 0.7830 - val_loss: 0.4864 - val_accuracy: 0.7680\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.4288 - accuracy: 0.7984 - val_loss: 0.4614 - val_accuracy: 0.7850\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.4222 - accuracy: 0.8037 - val_loss: 0.4587 - val_accuracy: 0.7895\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.4029 - accuracy: 0.8164 - val_loss: 0.4426 - val_accuracy: 0.7905\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.3955 - accuracy: 0.8191 - val_loss: 0.4722 - val_accuracy: 0.7845\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.3860 - accuracy: 0.8214 - val_loss: 0.4468 - val_accuracy: 0.7960\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.3721 - accuracy: 0.8338 - val_loss: 0.4416 - val_accuracy: 0.8050\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 66s 266ms/step - loss: 0.3679 - accuracy: 0.8286 - val_loss: 0.4605 - val_accuracy: 0.7970\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.3487 - accuracy: 0.8405 - val_loss: 0.4369 - val_accuracy: 0.8075\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.3386 - accuracy: 0.8461 - val_loss: 0.4565 - val_accuracy: 0.8035\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.3256 - accuracy: 0.8560 - val_loss: 0.4432 - val_accuracy: 0.8095\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 66s 265ms/step - loss: 0.3122 - accuracy: 0.8622 - val_loss: 0.4619 - val_accuracy: 0.8145\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 66s 266ms/step - loss: 0.3110 - accuracy: 0.8666 - val_loss: 0.4186 - val_accuracy: 0.8120\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.2801 - accuracy: 0.8813 - val_loss: 0.5013 - val_accuracy: 0.8125\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.2723 - accuracy: 0.8785 - val_loss: 0.4904 - val_accuracy: 0.8115\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 67s 266ms/step - loss: 0.2655 - accuracy: 0.8896 - val_loss: 0.4753 - val_accuracy: 0.8120\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 66s 266ms/step - loss: 0.2629 - accuracy: 0.8859 - val_loss: 0.4924 - val_accuracy: 0.8065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0831dad210>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64,64) )\n",
        "#convert the image into array\n",
        "test_image = image.img_to_array(test_image)\n",
        "#adding one dimension to the image, first dimension is for batch\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image/255.0)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1: #acess batch and single prediction as batch\n",
        "   prediction = 'dog'\n",
        "else:\n",
        "   prediction = 'cat' "
      ],
      "metadata": {
        "id": "ute-2LWiU0hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuZ4zZHfylpK",
        "outputId": "f74b5475-6e4d-4e16-d573-d7664052ed5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mx7VgX7R1q-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}